You said:

!pip install -q moviepy



import os, shutil, glob, math, cv2, numpy as np
from moviepy.video.io.VideoFileClip import VideoFileClip
from tqdm import tqdm
VIDEO_PATH = "/content/DHN_side_view1.mp4"
OUTPUT_ROOT = "Processed_Video"
TRAIN_PREFIX = "DHN_wagon"
SAMPLE_SEC = 0.5
MERGE_SEC = 1.0
MIN_SEG_SEC = 0.6
SAMPLE_FRAME_EVERY = 2
EDGE_OPEN_THRESHOLD = 0.07# Cell 3 â€” thumbnails + histogram-difference candidate generation
os.makedirs(OUTPUT_ROOT, exist_ok=True)
clip = VideoFileClip(VIDEO_PATH)
fps = clip.fps
duration = clip.duration
print(f"Video duration: {duration:.2f}s, fps: {fps}")

sample_times = np.arange(0, duration, SAMPLE_SEC)
thumbs_dir = os.path.join(OUTPUT_ROOT, "thumbs_for_splits")
shutil.rmtree(thumbs_dir, ignore_errors=True)
os.makedirs(thumbs_dir, exist_ok=True)

print("Saving thumbnails...")
sampled_frames = []
for i,t in enumerate(tqdm(sample_times)):
    frame = clip.get_frame(t)
    frame_bgr = cv2.cvtColor((frame*255).astype(np.uint8), cv2.COLOR_RGB2BGR)
    p = os.path.join(thumbs_dir, f"thumb_{i:04d}_{t:.2f}.jpg")
    cv2.imwrite(p, frame_bgr)
    sampled_frames.append((t,p))

# compute HSV hist per thumbnail and Bhattacharyya distance between consecutive thumbnails
def hist_hsv(img_path):
    img = cv2.imread(img_path)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h = cv2.calcHist([hsv], [0,1], None, [50,60], [0,180,0,256])
    cv2.normalize(h, h)
    return h.flatten()

hists = [hist_hsv(p) for _,p in sampled_frames]
dists = [cv2.compareHist(np.float32(hists[i]), np.float32(hists[i+1]), cv2.HISTCMP_BHATTACHARYYA)
         for i in range(len(hists)-1)]
d = np.array(dists)
mean,std = d.mean(), d.std()
threshold = mean + 1.0*std
candidates = [sample_times[i+1] for i in range(len(d)) if d[i] > threshold]


merged = []
for t in candidates:
    if not merged:
        merged.append(t)
    else:
        if t - merged[-1] < MERGE_SEC:
            merged[-1] = (merged[-1] + t)/2.0
        else:
            merged.append(t)
candidates = merged

print("Auto split candidates (seconds):")
print(candidates)
print("\n--> Thumbnails saved to:", thumbs_dir)
print("Inspect thumbnails (Files panel -> open 'thumbs_for_splits') and verify candidates.")

split_times = candidates.copy()
segments = []
start = 0.0
for t in split_times:
    segments.append((start,float(t)))
    start = float(t)
segments.append((start, duration))
print("Proposed segments:", len(segments))

valid_segments = []
for idx,(s,e) in enumerate(segments, start=1):
    if e - s < MIN_SEG_SEC:
        continue
    folder = os.path.join(OUTPUT_ROOT, f"{TRAIN_PREFIX}_{len(valid_segments)+1}")
    os.makedirs(folder, exist_ok=True)
    out_path = os.path.join(folder, f"{TRAIN_PREFIX}_{len(valid_segments)+1}.mp4")
    print(f"Saving segment {len(valid_segments)+1}: {s:.2f} -> {e:.2f}")
    clip.subclip(s,e).write_videofile(out_path, codec='libx264', audio=False, verbose=False, logger=None)
    valid_segments.append((s,e,folder,out_path))

print("Saved segments:", len(valid_segments))
print("If counts look wrong: re-run Cell 3 with adjusted thresholds or adjust MERGE_SEC / MIN_SEG_SEC.")
import base64

detected_boxes_per_segment = {} 
for seg_idx, (s,e,folder,video_file) in enumerate(valid_segments, start=1):
    jpgs = sorted(glob.glob(os.path.join(folder, f"{TRAIN_PREFIX}_{seg_idx}_*.jpg")))
    annotated_frames_dir = os.path.join(folder, "annotated_frames")
    shutil.rmtree(annotated_frames_dir, ignore_errors=True)
    os.makedirs(annotated_frames_dir, exist_ok=True)
    segment_boxes = []
    for i, p in enumerate(jpgs):
        img = cv2.imread(p)

        boxes = detect_doors_in_image(img, EDGE_OPEN_THRESHOLD)
        segment_boxes.append((p, boxes))

        annotated_img = img.copy() 
        for (x,y,ww,hh,label, conf) in boxes: 
            cv2.rectangle(annotated_img, (x,y), (x+ww, y+hh), (0,255,0), 2)
            cv2.putText(annotated_img, f"{label}: {conf:.2f}", (x, y-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2) 

        outp = os.path.join(annotated_frames_dir, f"{TRAIN_PREFIX}_{seg_idx}_{i+1:03d}_ann.jpg") 
        cv2.imwrite(outp, annotated_img)

    detected_boxes_per_segment[seg_idx] = segment_boxes 
    print(f"Processed and annotated {len(jpgs)} frames for coach {seg_idx} using YOLOv8. Annotated images saved to {annotated_frames_dir}.")
import base64

html_path = os.path.join(OUTPUT_ROOT, "DHN_report.html")

with open(html_path, "w") as f:
    f.write("<html><head><title>Door Detection Report</title></head><body>")
    f.write("<h1>Door Detection Report</h1>")
    f.write("<p>If the door open labels are appearing incorrectly, you can try adjusting the EDGE_OPEN_THRESHOLD value in Cell 2 and re-run cells 6 and 7.</p>")
    f.write("<p>Note: This report embeds images directly, which increases file size but allows offline viewing.</p>") # Added note about embedding
    f.write("<table border='1' cellspacing='0' cellpadding='5'>")
    f.write("<tr><th>Segment</th><th>Frame</th><th>Detected Objects</th><th>Image</th></tr>")

    for seg_idx, (s,e,folder,video_file) in enumerate(valid_segments, start=1):
        annotated_frames_dir = os.path.join(folder, "annotated_frames") # Directory where annotated frames are saved
        annotated_jpg_files = sorted(glob.glob(os.path.join(annotated_frames_dir, f"{TRAIN_PREFIX}_{seg_idx}_*_ann.jpg"))) # Look for annotated images


        segment_detection_results = detected_boxes_per_segment.get(seg_idx, [])


        for frame_idx, annotated_img_path in enumerate(annotated_jpg_files):

            if frame_idx < len(segment_detection_results):
                original_img_path, boxes = segment_detection_results[frame_idx]
                detected = ", ".join([f"{label} ({conf:.2f})" for (x,y,w,h,label, conf) in boxes]) if boxes else "None" # Include confidence
            else:
                detected = "Detection results not available"



            with open(annotated_img_path, "rb") as img_file:
                img_data = base64.b64encode(img_file.read()).decode('utf-8')
            img_src = f"data:image/jpeg;base64,{img_data}" # Assuming images are JPEGs

            f.write("<tr>")
            f.write(f"<td>{seg_idx}</td>")
            f.write(f"<td>{frame_idx + 1}</td>")
            f.write(f"<td>{detected}</td>") # Display detected objects with confidence
            f.write(f"<td><img src='{img_src}' width='320'></td>") # Use Data URI for annotated image
            f.write("</tr>")

    f.write("</table></body></html>")

print(f"âœ… HTML report saved to {html_path}")
zip_base = "DHN_processed_videos"
shutil.make_archive(zip_base, 'zip', OUTPUT_ROOT)
zip_path = zip_base + ".zip"
print("Zipped output at:", zip_path)
print("Files created under:", OUTPUT_ROOT)
print("Coach folders:", sorted([d for d in os.listdir(OUTPUT_ROOT) if d.startswith(TRAIN_PREFIX)][:50]))

zip_base = "DHN_processed_videos"
shutil.make_archive(zip_base, 'zip', OUTPUT_ROOT)
zip_path = zip_base + ".zip"
print("Zipped output at:", zip_path)
print("Files created under:", OUTPUT_ROOT)
print("Coach folders:", sorted([d for d in os.listdir(OUTPUT_ROOT) if d.startswith(TRAIN_PREFIX)][:50]))

zip_base = "DHN_processed_videos"
shutil.make_archive(zip_base, 'zip', OUTPUT_ROOT)
zip_path = zip_base + ".zip"
print("Zipped output at:", zip_path)
print("Files created under:", OUTPUT_ROOT)
print("Coach folders:", sorted([d for d in os.listdir(OUTPUT_ROOT) if d.startswith(TRAIN_PREFIX)][:50]))
import os

# Create the folder if not already
os.makedirs("Processed_Video", exist_ok=True)

# Placeholder for the actual object detection function
def detect_doors_in_image(image, threshold):
   
    return [] i need to explain m,y project for 5 minute video help to explain cell by cell 
ChatGPT said:

Got it ğŸ‘ You want to explain your train analysis project (video splitting + coach detection + annotated report generation) for a 5-minute video.
Iâ€™ll give you a simple, cell-by-cell script so you can present clearly.

ğŸ¥ Project Explanation Script (Cell by Cell)
Intro (30â€“40 sec)

ğŸ‘‰ Start with:
â€œThis project is about analyzing a train video taken from the side view.
The main goals are:

Split the video into segments, where each segment represents one train coach.

Detect and annotate doors in each coach using object detection.

Generate an HTML report with results.
Now I will explain the code step by step.â€

Cell 1 â€“ Imports and Setup

ğŸ‘‰ Say:
â€œIn this cell, I install MoviePy and import required libraries like cv2, numpy, and tqdm.

MoviePy is for handling videos.

cv2 is OpenCV, used for image processing.

numpy is for numerical operations.

tqdm adds progress bars.

I also set constants:

VIDEO_PATH = path of the train video.

OUTPUT_ROOT = folder where processed results will be saved.

Parameters like SAMPLE_SEC, MERGE_SEC, and MIN_SEG_SEC control how video is sampled and split.â€

Cell 2 â€“ Video Info & Thumbnails

ğŸ‘‰ Say:
â€œHere I open the video with VideoFileClip and print its FPS and duration.
I then sample frames every 0.5 seconds, convert them to images, and save them as thumbnails.
This helps us later to detect where one coach ends and another starts.â€

Cell 3 â€“ Histogram Difference & Split Candidates

ğŸ‘‰ Say:
â€œNow I calculate the HSV histogram of each thumbnail. HSV is better than RGB for color comparison.
I compute the Bhattacharyya distance between consecutive frames.
If the distance is high, it means a significant change â€” likely a new coach boundary.
I then merge nearby boundaries and print the candidate split times.â€

Cell 4 â€“ Segmenting the Video

ğŸ‘‰ Say:
â€œUsing the split times, I cut the video into multiple segments.
Each segment ideally corresponds to a single train coach.
If a segment is too short, itâ€™s discarded.
Finally, I save each segment as a separate video file under the Processed_Video folder.â€

Cell 5 â€“ Object Detection (Doors)

ğŸ‘‰ Say:
â€œNow for each segment, I process frames and run a placeholder function called detect_doors_in_image.
Right now, it just returns an empty list, but in practice we would use a model like YOLOv8.
The function returns bounding boxes for detected doors.
Detected doors are drawn on the frame with rectangles and labels.
I save these as annotated frames.â€

Cell 6 â€“ HTML Report Generation

ğŸ‘‰ Say:
â€œHere I generate an HTML report.
It contains a table with:

Segment number (coach number),

Frame number,

Detected objects with confidence,

Annotated image (embedded directly).

So even without internet, we can open the HTML and see results.â€

Cell 7 â€“ Zipping Results

ğŸ‘‰ Say:
â€œIn the last step, I zip the whole output folder into DHN_processed_videos.zip.
This makes it easy to share all processed videos, images, and reports together.â€

Closing (20â€“30 sec)

ğŸ‘‰ Say:
â€œTo summarize, this pipeline automatically:

Splits a train video into coaches,

Runs door detection,

Saves annotated results,

Generates a structured HTML report.

This approach can be extended to railway safety monitoring or automatic coach inventory systems.â€

âš¡ This script should cover ~5 minutes if you speak at a moderate pace.

Do you want me to also make a short presentation outline (slides) so you can record along with visuals instead of just screen-recording the notebook?

You said:
yes
ChatGPT said:

Perfect ğŸ‘ Letâ€™s make a short slide outline for your 5-minute presentation.
This way you can switch between slides + code demo while explaining.

ğŸ¤ 5-Minute Presentation Outline (Slides + Demo)
Slide 1 â€“ Title

Project Title: Automatic Train Coach Analysis from Side-View Video

Your Name & Date

Background image: blurred train photo

ğŸ‘‰ Script:
â€œHello, my project is about analyzing a moving train video from the side view. The goal is to automatically identify coaches, detect their doors, and generate a report.â€

Slide 2 â€“ Objectives

Split train video into individual coach clips

Detect and annotate coach doors

Generate structured HTML report

ğŸ‘‰ Script:
â€œThese are the three main objectives of my project.â€

Slide 3 â€“ Tools & Libraries

Python

MoviePy â€“ video handling

OpenCV â€“ image processing

NumPy â€“ computations

tqdm â€“ progress tracking

(Optional) YOLOv8 â€“ door detection

ğŸ‘‰ Script:
â€œThese are the tools and libraries I used.â€

Slide 4 â€“ Workflow

Import and Setup

Extract Thumbnails

Detect Coach Boundaries

Segment Video

Door Detection

Generate HTML Report

ğŸ‘‰ Script:
â€œThis is the step-by-step workflow.â€

Slide 5 â€“ Code Demo Part 1 (Setup + Thumbnails)

ğŸ‘‰ Show your Colab notebook Cell 1 & 2.
ğŸ‘‰ Script:
â€œFirst, I load the video, print its duration, and save thumbnails at fixed intervals.â€

Slide 6 â€“ Code Demo Part 2 (Splitting Coaches)

ğŸ‘‰ Show Cell 3 & 4.
ğŸ‘‰ Script:
â€œNext, I use histogram differences to detect where one coach ends and another starts.
Then I segment the video accordingly.â€

Slide 7 â€“ Code Demo Part 3 (Door Detection)

ğŸ‘‰ Show Cell 5.
ğŸ‘‰ Script:
â€œNow, I run door detection on each coach segment. For now, the function is a placeholder, but it can be replaced with YOLO for real detection.
Annotated frames are saved in each coach folder.â€

Slide 8 â€“ Code Demo Part 4 (Report + Results)

ğŸ‘‰ Show Cell 6 & 7.
ğŸ‘‰ Script:
â€œFinally, I generate an HTML report with detected results and zip all outputs.
The report shows segment number, frame, detected objects, and annotated image.â€

Slide 9 â€“ Applications

Railway safety monitoring

Automatic inventory of train coaches

Detecting anomalies like open doors

ğŸ‘‰ Script:
â€œThis project can be applied in real-world railway systems.â€

Slide 10 â€“ Conclusion

Automated pipeline for train coach analysis

Outputs: segmented clips + annotated images + HTML report

Future work: integrate real object detection model (YOLOv8)

ğŸ‘‰ Script:
â€œTo conclude, this pipeline fully automates train coach analysis and can be extended further.â€

âš¡ Suggested Timing:

Intro + Objectives: ~1 min

Workflow + Tools: ~1 min

Code Demo (Cells 1â€“7): ~2.5 min

Applications + Conclusion: ~0.5 min

Do you want me to design ready-to-use slides (with text + simple visuals) so you can directly present, or just keep this outline as your script?

You said:
i also have dhn minimal report
ChatGPT said:

Got it ğŸ‘ You already have a DHN minimal report (probably a shorter HTML/text report with key results).

Hereâ€™s how you can integrate that into your 5-minute presentation:

ğŸ”— How to Use the Minimal Report in Presentation
Slide 8 (Updated â€“ Results & Report)

Title: Results â€“ DHN Minimal Report

Show a screenshot of your DHN minimal report (table + sample annotated image).

Keep text short:

Segmented Coaches: X

Annotated Frames: âœ…

HTML Report Generated

ğŸ‘‰ Script:
â€œAlong with the full detailed HTML report, I also generated a DHN minimal report, which is a concise version.
It summarizes key outputs like the number of coaches, annotated sample frames, and detection results in a more compact format.
This makes it easier to quickly review the results without going through the full report.â€

Slide 9 (Applications)

ğŸ‘‰ After showing the minimal report, move to applications.
â€œThis minimal version is especially useful if railway staff or engineers need a quick overview without large file sizes.â€

âœ… This way, the detailed report shows completeness, and the minimal report shows efficiency.

Do you want me to rewrite your presentation script so that it mentions both the detailed HTML report and the minimal DHN report naturally in the flow?

You said:
Minimal Coverage Door Detection Report
Total coaches detected: 27

Coach 1
Clip: DHN_wagon_1.mp4 | Duration: 29.75s | Selected frames: 1


Coach 2
Clip: DHN_wagon_2.mp4 | Duration: 10.25s | Selected frames: 1


Coach 3
Clip: DHN_wagon_3.mp4 | Duration: 6.00s | Selected frames: 1


Coach 4
Clip: DHN_wagon_4.mp4 | Duration: 4.00s | Selected frames: 1


Coach 5
Clip: DHN_wagon_5.mp4 | Duration: 2.50s | Selected frames: 1


Coach 6
Clip: DHN_wagon_6.mp4 | Duration: 10.25s | Selected frames: 1


Coach 7
Clip: DHN_wagon_7.mp4 | Duration: 9.25s | Selected frames: 1


Coach 8
Clip: DHN_wagon_8.mp4 | Duration: 6.00s | Selected frames: 1


Coach 9
Clip: DHN_wagon_9.mp4 | Duration: 10.00s | Selected frames: 1


Coach 10
Clip: DHN_wagon_10.mp4 | Duration: 1.50s | Selected frames: 1


Coach 11
Clip: DHN_wagon_11.mp4 | Duration: 1.50s | Selected frames: 1


Coach 12
Clip: DHN_wagon_12.mp4 | Duration: 12.50s | Selected frames: 1


Coach 13
Clip: DHN_wagon_13.mp4 | Duration: 3.00s | Selected frames: 1


Coach 14
Clip: DHN_wagon_14.mp4 | Duration: 1.50s | Selected frames: 1


Coach 15
ChatGPT said:

Perfect ğŸ‘ You already have your Minimal Coverage Door Detection Report with coach-wise details.
We can highlight this in your 5-minute explanation to show summary + efficiency.

Hereâ€™s how to weave it in:

ğŸ¤ Updated Presentation Script (with Minimal Report)
Slide 8 â€“ Results (Detailed + Minimal Report)

Show screenshot of full HTML report (with annotated images).

Script:
â€œThis is the detailed HTML report. It includes every segment, annotated images, and detection details. Itâ€™s useful for in-depth analysis.â€

Slide 9 â€“ Minimal Report (Summary)

Title: Minimal Coverage Report

Show screenshot/snippet like this:

Total coaches detected: 27  

Coach 1 â†’ Duration: 29.75s  
Coach 2 â†’ Duration: 10.25s  
Coach 3 â†’ Duration: 6.00s  
...  


Script:
â€œI also generate a minimal report.
This focuses only on key statistics:

Total coaches detected

Duration of each coach clip

Selected sample frames

For example, in this run, the system detected 27 coaches.
Each coach clip has its own duration, and at least 1 frame was selected for inspection.â€

Slide 10 â€“ Applications

Railway staff can use the detailed report for deep inspection.

The minimal report is better for quick checks, monitoring, or when file size needs to stay small.

ğŸ‘‰ Script:
â€œTogether, these two reports provide both detail and efficiency. The detailed one helps engineers, while the minimal one helps in quick monitoring.â€

âš¡ By showing both, you demonstrate that your system is flexible: it can produce heavy detailed analysis as well as lightweight summaries.

Would you like me to also design a polished slide template (with tables, icons, colors) that you can use in your presentation â€” so your minimal report doesnâ€™t look like plain text but like a structured result table?

You said:
write readme.d for github
ChatGPT said:

Got it ğŸ‘ You want a README.md for GitHub that clearly explains your project.
Hereâ€™s a polished draft you can use (Markdown formatted):

ğŸš† Train Coach Analysis & Door Detection

This project processes a side-view video of a moving train to automatically:

Split the video into individual coach clips

Detect and annotate train doors (placeholder / extendable with YOLOv8)

Generate detailed and minimal reports summarizing results

ğŸ“‚ Project Workflow

Setup & Imports

Uses Python, OpenCV, MoviePy, NumPy, tqdm

Thumbnail Extraction

Sample frames at fixed intervals

Save as thumbnails for boundary detection

Coach Boundary Detection

Use HSV histograms + Bhattacharyya distance to find coach boundaries

Merge close boundaries

Video Segmentation

Split the train video into multiple clips (each representing a coach)

Door Detection (Placeholder)

Function detect_doors_in_image() currently returns empty results

Can be replaced with YOLOv8 or another object detection model

Report Generation

Detailed HTML Report â†’ includes annotated frames with bounding boxes

Minimal Coverage Report â†’ summary of total coaches, clip durations, and sample frames

Packaging

Final outputs zipped for easy sharing

ğŸ“Š Example Minimal Coverage Report
Minimal Coverage Door Detection Report  
Total coaches detected: 27  

Coach 1 â†’ Duration: 29.75s | Frames: 1  
Coach 2 â†’ Duration: 10.25s | Frames: 1  
Coach 3 â†’ Duration: 6.00s | Frames: 1  
...  

ğŸ› ï¸ Tech Stack

Python 3.x

MoviePy
 â€“ video handling

OpenCV
 â€“ image processing

NumPy
 â€“ numerical operations

tqdm
 â€“ progress bars

(Optional) YOLOv8
 â€“ for real object detection
 ğŸ“Œ Applications

Railway safety monitoring

Automatic inventory of train coaches

Detecting anomalies like open doors

Video-based inspection systems

ğŸ“ˆ Future Work

Replace placeholder detection with YOLOv8 for real door detection

Optimize coach boundary detection with deep learning models

Deploy as a web dashboard for live monitoring

ğŸ‘¤ Author

Aryan Tyagi

ğŸ“ B.Tech CSE (AI & ML) | SRM Institute of Science and Technology
